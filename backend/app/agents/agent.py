"""
This module defines the supervisor agent and its
execution flow using LangGraph.
"""

import asyncio
import json
import operator
from typing import Annotated, TypedDict, Union

from langchain.memory import ConversationBufferMemory
from langchain_community.utilities import WikipediaAPIWrapper
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, Field, ValidationError

from backend.app.agents.rag_memory import (
    generate_response,
    initialize_rag_chat_chain
)
from backend.app.utils import get_model


class FinalAnswer(BaseModel):
    """Final Answer: The response is of high quality and ready for the user."""

    answer: str = Field(
        ...,
        description="Final answer is clear and well structured.",
    )


class CorrectAndRefine(BaseModel):
    """Decision: The response is correct but needs improvement."""

    reasoning: str = Field(
        ...,
        description="Explaination of why the response needs refinement.",
    )
    corrected_answer: str = Field(
        ..., description="The improved and corrected version of the response."
    )


class ComplementWithWikipedia(BaseModel):
    """
    Decision: The response would benefit from extra context from Wikipedia.
    """

    reasoning: str = Field(
        ..., description="Explanation of why addittional context is needed."
    )
    search_query: str = Field(
        ..., description="Well-formed search query for Wikipedia."
    )


SupervisorDecision = Union[
    FinalAnswer, CorrectAndRefine, ComplementWithWikipedia
]


class GraphState(TypedDict):
    """
    Defines the state that flows through the LangGrach graph.
    """

    user_question: str
    rag_answer: str
    supervisor_decision: SupervisorDecision
    final_answer: str
    revision_count: Annotated[int, operator.add]


class RefinementAgent:
    """
    Agent that evaluates and refines the response.
    """

    def __init__(self):
        """
        Initializes the RefinementAgent by loading the language model (LLM).
        """
        self.llm = get_model()

    def _create_supervisor_prompt(
        self,
        user_question: str,
        rag_answer: str
    ) -> str:
        """
        Create the prompt for the LLM supervisor, instructing it to
        return a structured decision in JSON format.

        Args:
            user_question (str): User message.
            rag_answer (str): RAG response.

        Returns:
            str: Prompt well-formated for a language model.
        """
        return f"""
        Tu rol es ser un Supervisor de Calidad de IA. Analiza la respuesta
        del RAG y decide el siguiente paso. Debes responder **SOLO** con
        un objeto JSON que contenga dos campos: "type" y "data".

        El campo "type" debe ser uno de los siguientes strings:
        - "FinalAnswer" (si la respuesta del RAG es excelente y está
           lista para el usuario)
        - "CorrectAndRefine" (si la respuesta del RAG es correcta pero
          necesita mejoras de estilo o claridad)
        - "ComplementWithWikipedia" (si la respuesta del RAG es buena
          pero se beneficiaría de contexto adicional de Wikipedia)

        El campo "data" debe ser un JSON que corresponda al tipo elegido:

        Si "type" es "FinalAnswer":
        {{
            "answer": "La respuesta final, clara y bien formada, para ser
                      mostrada directamente al usuario."
        }}

        Si "type" es "CorrectAndRefine":
        {{
            "reasoning": "Explicación concisa de por qué la respuesta
                         necesita ser refinada.",
            "corrected_answer": "La versión mejorada y corregida de la
                                respuesta."
        }}

        Si "type" es "ComplementWithWikipedia":
        {{
            "reasoning": "Explicación de por qué se necesita contexto
                         adicional y qué se va a buscar.",
            "search_query": "La consulta de búsqueda optimizada para
                            Wikipedia (ej. 'Economic Order Quantity')."
        }}

        **Ejemplo de respuesta JSON:**
        ```json
        {{
            "type": "FinalAnswer",
            "data": {{
                "answer": "El Just-in-Time es una filosofía de producción
                          que busca eliminar el desperdicio..."
            }}
        }}
        ```

        **Pregunta Original:** "{user_question}"
        **Respuesta del RAG:** "{rag_answer}"

        Tu respuesta JSON:
        """

    async def review_answer(
        self, user_question: str, rag_answer: str
    ) -> SupervisorDecision:
        """
        Reviews the answer generated by the RAG agent and
        decides the action to follow, eather approve, refine
        or complement with Wikipedia.

        Args:
            user_question (str): User message.
            rag_answer (str): RAG response.

        Returns:
            SupervisorDecision: FinalAnswer, CorrectAndRefine
                                or ComplementWithWikipedia.
        """
        prompt = self._create_supervisor_prompt(user_question, rag_answer)

        llm_response = await self.llm.ainvoke(prompt)
        response_content = llm_response.content.strip()

        if (
            response_content.startswith("```json")
            and response_content.endswith("```")
        ):
            response_content = response_content[
                len("```json"):-len("```")
            ].strip()

        try:
            parsed_json = json.loads(response_content)

            response_type = parsed_json.get("type")
            response_data = parsed_json.get("data")

            if response_type == "FinalAnswer":
                return FinalAnswer(**response_data)
            elif response_type == "CorrectAndRefine":
                return CorrectAndRefine(**response_data)
            elif response_type == "ComplementWithWikipedia":
                return ComplementWithWikipedia(**response_data)
            else:
                print(
                    f"Advertencia:"
                    f"Tipo de decisión no reconocido: {response_type}."
                    f"Devolviendo respuesta original."
                )
                return FinalAnswer(
                    answer=f"""Hubo un problema al procesar la respuesta.
                    Tipo de decisión no reconocido: {response_type}.
                    Respuesta original: {rag_answer}
                    """
                )

        except json.JSONDecodeError:
            print(
                f"Error: Respuesta de LLM no es un JSON válido."
                f"Respuesta: {response_content}"
            )
            return FinalAnswer(
                answer=f"""Hubo un problema al procesar la respuesta.
                El LLM no devolvió un JSON válido.
                Respuesta original: {rag_answer}
                """
            )
        except ValidationError as e:
            print(
                f"Error validación Pydantic: {e}."
                f"Respuesta: {response_content}"
            )
            return FinalAnswer(
                answer=f"""Hubo un problema al procesar la respuesta.
                Error de validación: {e}.
                Respuesta original: {rag_answer}
                """
            )
        except Exception as e:
            print(
                f"Error inesperado al procesar la respuesta del LLM: {e}."
                f"Respuesta: {response_content}"
            )
            return FinalAnswer(
                answer=f"""Hubo un problema al procesar la respuesta.
                Error: {e}.
                Respuesta original: {rag_answer}
                """
            )

    async def combine_with_wikipedia(
        self, original_answer: str, wiki_context: str
    ) -> str:
        """
        Combines the original RAG answer with additional context
        obtained from Wikipedia.

        Args:
            original_answer (str): The original RAG answer.
            wiki_context (str): The context text obtained from Wikipedia.

        Returns:
            str: The final enriched answer.
        """
        prompt = f"""
        Combina la respuesta original con el contexto
        de Wikipedia de forma natural.

        Respuesta Original: "{original_answer}"
        Contexto Wikipedia: "{wiki_context}"
        Respuesta Combinada:
        """
        response = await self.llm.ainvoke(prompt)
        return response.content


refinement_agent = RefinementAgent()


async def call_rag_agent(state: GraphState) -> dict:
    """Node that invokes the RAG agent to get the initial response."""
    print("--- Nodo: Call RAG Agent ---")
    user_question = state["user_question"]
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    rag_chain = initialize_rag_chat_chain(memory)
    response = await generate_response(
        rag_chain,
        user_question,
        "langgraph_session"
    )
    return {"rag_answer": response, "revision_count": 1}


async def call_supervisor_agent(state: GraphState) -> dict:
    """Node that invokes the supervisor agent to evaluate the RAG response."""
    print("--- Nodo: Call Supervisor Agent ---")
    user_question = state["user_question"]
    rag_answer = state["rag_answer"]
    decision = await refinement_agent.review_answer(user_question, rag_answer)
    print(f"   -> Decisión: {type(decision).__name__}")
    return {"supervisor_decision": decision}


async def enrich_with_wikipedia(state: GraphState) -> dict:
    """Node that enriches the response with information from Wikipedia."""
    print("--- Nodo: Enrich with Wikipedia ---")
    decision = state["supervisor_decision"]
    rag_answer = state["rag_answer"]

    print(f"   -> Buscando en Wikipedia: '{decision.search_query}'")
    wiki_tool = WikipediaAPIWrapper(
        top_k_results=1,
        doc_content_chars_max=2500
    )
    wiki_context = await asyncio.to_thread(
        wiki_tool.run,
        decision.search_query
    )

    print("   -> Combinando respuestas...")
    final_answer = await refinement_agent.combine_with_wikipedia(
        rag_answer, wiki_context
    )
    return {"final_answer": final_answer}


async def prepare_final_response(state: GraphState) -> dict:
    """Node that prepares the final response when Wikipedia is not required."""
    print("--- Nodo: Prepare Final Response ---")
    decision = state["supervisor_decision"]
    if isinstance(decision, FinalAnswer):
        print("   -> Acción: Aprobar respuesta.")
        return {"final_answer": decision.answer}
    elif isinstance(decision, CorrectAndRefine):
        print(
            f"   -> Acción: Aplicar refinamiento."
            f"Razón: {decision.reasoning}"
        )
        return {"final_answer": decision.corrected_answer}
    return {}


def route_decision(state: GraphState) -> str:
    """Routing that decides the next step based on the supervisor's decision"""
    print("--- Router: Route Decision ---")
    decision = state["supervisor_decision"]
    if isinstance(decision, ComplementWithWikipedia):
        print("   -> Ruta: a enrich_with_wikipedia")
        return "enrich_with_wikipedia"
    else:
        print("   -> Ruta: a prepare_final_response")
        return "prepare_final_response"


def build_graph():
    """Construct and compile the LangGraph workflow."""
    workflow = StateGraph(GraphState)

    workflow.add_node("call_rag_agent", call_rag_agent)
    workflow.add_node("call_supervisor_agent", call_supervisor_agent)
    workflow.add_node("enrich_with_wikipedia", enrich_with_wikipedia)
    workflow.add_node("prepare_final_response", prepare_final_response)

    workflow.set_entry_point("call_rag_agent")
    workflow.add_edge("call_rag_agent", "call_supervisor_agent")
    workflow.add_conditional_edges(
        "call_supervisor_agent",
        route_decision,
        {
            "enrich_with_wikipedia": "enrich_with_wikipedia",
            "prepare_final_response": "prepare_final_response",
        },
    )
    workflow.add_edge("enrich_with_wikipedia", END)
    workflow.add_edge("prepare_final_response", END)

    return workflow.compile()


async def process_user_question(user_question: str) -> str:
    """
    Process the user's question through the LangGraph flow
    and return the final answer.

    Args:
        user_question (str): User's question.

    Returns:
        str: Final answer generated by the chatbot.
    """
    print("🚀 Iniciando el flujo con LangGraph...")
    app = build_graph()

    inputs = {"user_question": user_question}

    final_state = await app.ainvoke(inputs)

    return final_state["final_answer"]
